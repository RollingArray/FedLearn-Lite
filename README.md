# FedLearn-Lite: Federated Learning for Linear Regression

A lightweight yet powerful federated learning simulation using Flower and TensorFlow â€” where multiple clients collaboratively train a shared global model without ever sharing their raw data.

> ðŸ“Œ Perfect for demonstrating how decentralized learning can effectively learn a simple linear relationship: `y = 2x + 1`

---

## ðŸš€ Overview

`FedLearn-Lite` simulates a **realistic federated learning setup** across multiple clients. Each client owns a subset of data and trains a local model. A central server performs **Federated Averaging (FedAvg)** to aggregate the weights into a global model.

The project demonstrates:

- How **federated learning** works in a practical client-server architecture.
- How to split and distribute data across clients to preserve privacy.
- How a global model can learn a perfect linear mapping `y = 2x + 1` without any data centralization.
- Evaluation of global model after multiple rounds of learning.

---

## ðŸ§  Technologies Used

- [Flower](https://flower.dev) â€” Federated Learning Framework
- [TensorFlow](https://tensorflow.org) â€” Deep Learning Engine
- Python 3.x

---

## ðŸ—‚ï¸ Project Structure

```text
FedLearn-Lite/
â”‚
â”œâ”€â”€ data/                     # Client-specific datasets
â”‚   â”œâ”€â”€ client-data-1.csv
â”‚   â”œâ”€â”€ client-data-2.csv
â”‚   â””â”€â”€ client-data-3.csv
â”‚
â”œâ”€â”€ weights/                  # Saved client model weights
â”‚
â”œâ”€â”€ model/                    # Saved global models after each round
â”‚
â”œâ”€â”€ plots/                    # Output visualizations (Actual vs Predicted)
â”‚
â”œâ”€â”€ model.py                  # Model architecture (Keras)
â”œâ”€â”€ generate_data.py          # Script to generate and split data for clients
â”œâ”€â”€ client.py                 # Federated client logic
â”œâ”€â”€ server.py                 # Flower server with FedAvg strategy
â”œâ”€â”€ test_model.py             # Evaluate and visualize final model
â””â”€â”€ README.md
````

---

## ðŸ§ª How to Run

### 1ï¸âƒ£ Generate Sample Data

```bash
python generate_data.py
```

This will create a synthetic dataset for the function `y = 2x + 1` with slight noise and split it across 3 clients.

---

### 2ï¸âƒ£ Start the Server

```bash
python server.py
```

This will:

* Launch the federated learning server
* Run for 5 federated rounds
* Save global model after each round
* Print performance on validation

---

### 3ï¸âƒ£ Start Clients (in 3 separate terminals or background processes)

```bash
CLIENT_ID=1 python client.py
CLIENT_ID=2 python client.py
CLIENT_ID=3 python client.py
```

Each client trains on its own dataset and communicates only weights with the server.

---

### 4ï¸âƒ£ Visualize Final Model

```bash
python test_model.py
```

This will:

* Load the final model (`global_model_round_5.h5`)
* Predict for a range of x values
* Plot both actual and predicted values
* Save the plot to `./plots/global_model_vs_actual.png`

---

## ðŸ“ˆ Sample Output

![Global Model Output](./plots/global_model_vs_actual.png)

---

## ðŸ” Privacy by Design

Unlike traditional ML, data **never leaves the client**. Only model weights are exchanged. This setup emulates a real-world federated scenario suitable for edge devices, IoT, healthcare, and finance domains.

---

## ðŸ§© Future Extensions

* Add support for non-linear functions and models
* Extend to image or tabular classification
* Add differential privacy or secure aggregation
* Enable real-time client participation via sockets or REST

---

## ðŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change or extend.

---

## ðŸ“œ License

This project is licensed under the MIT License.

---

## âœ¨ Acknowledgements

Thanks to the [Flower team](https://flower.dev) and TensorFlow community for building incredible tools for federated and distributed machine learning.

---

> "Federated Learning isnâ€™t the future. Itâ€™s the present â€” distributed, private, and smarter."

```
